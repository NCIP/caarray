Next generation sequencing data

Assay types:

Next generation sequencing experiments can be of different assay types: RNA-Seq, CHIP-Seq, Methylation, miRNA-Seq.
RNA-Seq and CHIP-Seq are the most prevalent assay types. RNA-Seq experiments attempt to find SNPs/mutations from known genes, discover novel genes, discover novel splicing events, or measure gene expression levels. CHIP-Seq experiments attempt to answer the question, "Where does my protein bind to?".


Sequencers and hardware:

The Illumina Genome Analyzer and ABI SOLiD are the most popular sequencers. Some centres also use the Roche 454.


Tools and File formats:

Sequencers produce files with short reads. The Illumina GA produces short read files in fastq, fasta and ELAND formats. ABI SOLiD produces short reads in csfasta format. There are typically 10-500 million reads in a file, each read about 50-100 base pairs long.

The reads are aligned either to a reference genome or de novo, and tools like BLAT and TopHat are used to do this alignment. SAM files are produced as a result. The Tuxedo tools from UMD are TopHat (map reads to reference genome and generate all possible splice junctions), BowTie (map reads contiguously) and 

SAM files can be converted to a more-efficient binary BAM format using SAMtools (which can also sort, index and visualize the BAM). A tool like Cufflinks (part of the Tuxedo tools along with TopHat) can take the BAM alignments and do isoform assembly and quantitation. The output from Cufflinks is a GTF file containing transfrags and attributes.

SAMtools, Broad IGV or CLC Bio Genomics Workbench allow visualization of how reads align to the reference genome, and how they overlap with known features on the reference. These tools are used to find SNPs, insertions and deletions.
TCGA uses a standard format (.maf) to report differences between reads and reference.
Galaxy has pipelines for next gen sequencing analysis.
Geospiza: Edge has been using it - interface not great. They target labs or companies like EdgeBio.
GMQuest/GenomeQuest? - cloud-based storage and provide web interface for visualization & analysis. They target labs or companies like EdgeBio.

Gene expression measurements are in terms of RPKM (Reads per kilobase of exon model per Million total reads in sample).

For CHIP-Seq experiments, there are several peak-calling algorithms like MACS, CisGenome, ERANGE, QuEST, FindPeaks, Sissrs etc.


MINSEQE:

A draft proposal for the required Minimum Information about a high-throughput Nucleotide SeQuencing Experiment - MINSEQE - specifies the following required elements:
1. Sample annotation and experimental factors.
2. The complete set of read sequences for each assay as generated by the sequencing instrument, in a recognized format, with quality scores, raw intensities and processing parameters for the instrument.
3. The 'final' processed (or summary) data for the set of assays in the study - i.e., the data on which the conclusions in the related publication are based. This would typically be a matrix with genes/exons against samples, with the values being transcript abundance etc.
4. The experiment design including sample data relationships
5. General information about the experiment, its goals, contact information and publications.
6. Essential experimental and data processing protocols. For example, the algorithm used to align the reads against the genome should be named and the associated software's version number, run parameters, and genome assembly version captured.


Case studies:

The 2010 LSBAM NGS working group participants:
* Simon Cancer Center (Indiana Univ.) - Ganesh Shankar
* Cancer Institute of NJ - Ryan Golhar
* Baylor College of Medicine - Lauren Boyd
* Jackson Labs - Grace Stafford
* Univ of Texas, Austin - Scott Hunicke-Smith
* Univ of Michigan - Jason Hipp
* UCLA - Buddy Dennis
* The TCGA program - Carl Schaeffer

1. Simon Cancer Center (Indiana Univ.):
* Centres: NGS core does 1st analysis - base calling. Bioinformatics centre  does data mgmt and 2nd analysis - alignment, SNP calling. University IT dept. provides storage and compute cluster. Other institutions (Notre Dame, Purdue) are customers.
* Assay types: resequencing to find SNPs/CNV, RNA-seq, CHIP-seq. (de novo sequencing and investigatory expos on non-human organisms. Biomarker discovery in mouse/rat models.)
* Sequencer: ABI SOLiD 4, Illumina GAII, Roche 454
* Sample prep: DON'T KNOW / NOT STANDARD
* File formats: .qual, .csfasta, BAM
* Hardware: Quarry is a seven teraflop system built from Intel processors. Quarry consists of 140 IBM HS21 Blade servers, each containing two Intel Xeon 5335 quad-core processors. (http://kb.iu.edu/data/avju.html). Files are stored on the IBM General Parallel File System (GPFS) - a shared-disk clustered file system - with a capacity of 266TB.
* Software: Initial processing & alignment done using shell scripts, bfast, SAMtools and Partek. Further analysis & aggregation done using custom R algorithms. MyTrack lets them track experiments, samples, data and integrates with the UCSC genome browser for visualization.
* Data flow: Data goes from sequencing core to bioinformatics core to customer/user. Raw image is kept for 2 weeks, aligned data (BAM) is kept for 1 year.
* Challenges: network bandwidth, FTP server (shipping data back to the customers?), large memory and disk space requirements.

2. Cancer Institute of NJ:
* Centres: Sequencing cores at Rutgers and Princeton Univ. sequence and send the data to the bioinformatics core at CINJ.
* Assay types: RNA-Seq, CHIP-seq, whole genome analysis.
* Sequencers: ?
* Sample prep: PIs do sample prep and hand off to core for sequencing.
* File formats: Csfasta/qual, fastq, SAM/BAM
* Hardware: 56-node Linux cluster with 22TB storage. 300-node Linux cluster for high-throughput analysis (16 GB/core).
* Software: Pipelines for RNA-Seq, and ChIP-Seq experiments. Future pipeline for whole-genome analysis. They use a variety of open-source tools to perform analysis in addition to Ingenuity Pathway Analysis. IGV, Gbrowse, UCSC Genome Browser.
* Challenges: Tools to store and manage data, similar to caArray. (A good model to follow would be something like Galaxy.) Need an integrated pipeline that is easy to use and customize for different organisms, questions, etc. How store genome data for patients in EMR?

3. Baylor College of Medicine:
* Centres: "Human Genome Sequencing Centre", "Microarray Core Facility", PI labs.
* Assay types: RNA-seq, CHIP-seq, resequencing, SV, resequencing, SNP discovery, methylation, WGS
* Sequencers: Illumina GAIIx, HiSeq 2000, Roche 454, ABI SOLiD
* Sample prep: Full service. Sample prep, extraction and QC is done by the Human Genome Sequencing Centre and Microarray Core Facility.
* File formats: .sff, .tiff, FASTA, FASTQ, XML library and sequencer metadata files, .bam, .sam, .xml, .xls, tab-delimited text
* Hardware: (1) Sequencing core: ? (2) PIs in Cancer Center: Storage & analysis on Cluster w/ 2TB head node, 32 250GB compute nodes half with Dual-Core 2216 (2.4GHz) with 4GB RAM & half with Quad-Core 2354 (2.2GHz) with 16GB RAM/Storage Pillar w/ 26 TB in four bricks Ð all Linux; Other PIs: Miscellaneous large RAM, TB storage data rigs
* Software: Sequencing centres use Newbler, Genboree, BLAST, BowTie, Glimmer & GeneMark (gene prediction), RAST annotation, Qiime & MOTHUR (HMP), in-house software (e.g., .sff-> .srf & upload to SRA), SHRiMP (tools depend on project, pipelines established), tRNAScan, RNAmmer, Rfam (RNA prediction), BLASTp, HMMER (gene function annotations). PIs use Qiime, Excel, R w/ Vegan pkg (HMP), BRB Array Tools, Galaxy, R/Bioconductor, GeneSpring, custom software, Genepattern, IGV.
* Data flow: Data is monitored in custom LIMS from StoneBond, and may upload to data analysis & coordination center (DACC) if a multi-inst. study. Customers download data via FTP. The DACC manages data in SVN repositories.
* Challenges: Data Analysis Coordinating Center (DACC) is slow; in-house analysis begins a long time before the data is released to the public.

4. Jackson Labs:
* Centres: Gene expression center (sample prep and running the arrays); Stats & Analysis (alignment & analysis); some PIs handle analysis.
* Assay types: RNA-Seq, CHIP-Seq, resequencing, methylation, microRNA. Biomarker discovery/validation, mutation discovery.
* Sequencers: Illumina Genome Analyzer IIX, soon upgrading to HiSeq2000
* Sample prep: Gene expression center or PI does sample prep.
* File formats: .jpg images temporarily stored, fastq delivered to customer, SAM/BAM.
* Hardware: Isilon for storage (expensive), cluster for analysis.
* Software: Bowtie and TopHat, IGV, still exploring analysis tools (Galaxy, GenomeQuest, MACS).
* Data flow: Genologics for LIMS
* Challenges: Need canned pipelines for routine analysis.

5. University of Texas, Austin:
* Centres: Core lab, ENCODE (PI) lab. Core lab and PI lab do sample prep and QC, as well as alignment and analysis.
* Assay types: RNA-Seq, CHIP-Seq, resequencing, SV; moving towards methylation and some microRNA. Biomarker discovery/validation.
* Sequencers: ABI SOLiD version 4, Roche 454.
* Sample prep: Core lab and PI lab do sample prep and QC.
* File formats: .csfasta & _QV.qual (SOLiD files), .bam (alignments), .wig (visualization of alignments), .fna and .sff (454 files), .tsv, .ace, and (mainly) .txt files (454 alignment results).
* Hardware: Local server = Quad-processor quad-core Dell R900Õs with large RAID arrays (6TB for core, 60TB for ENCODE lab). University-wide supercomputer = 4,000 nodes, 63,000 cores, 1.7 PB disk.
* Software: BWA aligner and custom peak finding software, visualization by UCSC genome browser, Shrimp aligner, maq, eland, custom scripts. Moving towards ABI BioScope pipeline.
* Data flow: Core lab backs up raw data to UniversityÕs central 10 PB tape storage and to local server; does mapping using ABI's BioScope pipeline either on local server or supercomputer; analysis done on local server; email location of results on local server to end-user. ENCODE lab receives reads and archives on local server, aligns, prepares files & job scripts for supercomputer, moves files to supercomputer, execute batch jobs, move data back to local server, create visualization.
* Challenges: Cost, turn-around time.

6. University of Michigan:
* Centres: Sequencing core lab, bioinformatics & computation centre, PIs. Core runs sequencer and does base calling and mapping. Analysis done by bioinformaticians in PI labs, or by core.
* Assay types: 80% whole genome sequencing (shallow 2x), 15% mRNA-Seq, 5% CHIP-Seq, miRNA-Seq, primer-specific sequencing, very little methyl-seq or deep-seq. Human sequence mutations, de novo sequencing of microorganisms.
* Sequencers: 4 * Illumina GA-IIx (replace with High Seq); 2 * ABI SOLID 3; 1 * 454 (rarely used because costly)
* Sample prep: PI does it.
* File formats: TIFFs (kept for 1 month), whole pipeline given to customer. fastQ, BAM/SAM, BED format
* Hardware: 7-node Dell cluster for mapping/aligning, 4 servers with 100 TB storage, 90TB storage off-campus. Data storage on central data repository, annotation data at NCBI, EBI, UCSC.
* Software: Illumina pipeline (Bustard Firecrest, GERALD); Roche pipeline for 454. ERANGE, Bowtie, Bed format -> UCSC browser, Gene lists -> ConceptGen
* Data flow: custom LIMS. Core runs sequencer and does base calling and mapping. Analysis done by bioinformaticians in PI labs, or by core. Core generates 2 terabytes a day.
* Challenges: Pace of change (new software, new instruments); disk space. Focusing on automation & robotics

7. UCLA:
* Centres: Genomics center, gene expression core, UCLA genotyping/sequencing core.
* Assay types:
* Sequencers: ABI SOLiD, Illumina Genome Analyzer, 454 (they don't use this)
* Sample prep:
* File formats:
* Hardware:
* Software:
* Data flow: Sequencer -> moved to Sun ZFS storage -> copied to Sun Lustre distributed storage -> aligned on a Linux cluster -> aligned data copied to Sun ZFS storage -> aligned data copied to off-site Sun ZFS storage. SAM files analyzed by bioinformaticians on their desktop. 1-5 TB per machine run generated. But 1Gbps Ethernet can only push ~ 2 TB /day.
* Challenges: Performance (1Gbps Ethernet can only push ~ 2 TB /day).

8. TCGA data:
+ DNA sequencing data from 3 big sequencing institutes: WashU, Broad, Baylor (Illumina, SOLiD)
+ miRNA sequencing data from British Columbia Cancer Agency (Illumina)
+ mRNA sequencing data from UNC (coming soon) (Illumina)

Low-level data goes to NCBI SRA: (protected data because genotype can be inferred)
+ bam aligned reads (2300 files = 50TB)
+ optionally, fastq/SRF pre-aligned reads --- more permanent representation since alignments can become dated, but bam is more convenient
+ metadata still being worked out
+ NCBI not yet accessioning bams fully
+ TCGA DCC getting biweekly updates from NCBI

Higher-level data goes to TCGA DCC: validated somatic mutations = public, but candidate mutations or low-level germline mutations from which person genotype could be inferred = protected
+ .maf file for variants (mutation calling)
+ wig file for coverage (binary) of probed exon location (tells if coverage was adequate or not - way to measure confidence)
+ abundance of mRNA, miRNA É. precise formats still being worked out, but will cover abundance (read coverage) at the level of exon, gene and exon junctions

https://wiki.nci.nih.gov/display/TCGA/BAM+File+Telemetry:
+ Up to date view of number of samples from each sequencing institute per disease (5 diseases)
+ Increasing at the rate of 185 new samples (10 TB) per month.
+ Data download uses Aspera (commercial product for which NCBI has a license) Can't use http or ftp (which are based on TCP/IP). Aspera is built on UDP. Server blasts out packets as fast as it can. Client's job to call duplicate packets and reassemble packets into the right order.


Other Repositories:

1. GEO data:

* RNA-Seq expt GSE15596: Raw files in GFF and WIG formats
* miRNA/siRNA expt GSE14462: Raw files in fasta format
* CHIP-Seq expt. GSE17312: Raw files in BAM and WIG formats
* Methylation-seq expt. GSE14966: Raw files in BED format
* Bisulfite sequencing / methylation expt. GSE11034: custom text formats
* Digital gene expr profiling expt. GSE14782: MAGE-ML (xml) data and annotations

Requirements for processed data files are not yet fully standardized and will depend on the nature of the experiment. Multiple types and levels of processed data files per sample can be accepted, for example, a ChIP-seq experiment would typically have alignment files (e.g., bed format) and peak files (e.g., wig format). A miRNA profiling experiment would typically have filtered, unique sequence reads with counts and possibly a wig file.

2. ArrayExpress & EBI data:

The European Nucleotide Archive (ENA) captures and presents information relating to experimental workflows that are based around nucleotide sequencing. ENA is made up of a number of distinct databases that includes EMBL-Bank, Sequence Read Archive (SRA) and the Trace Archive. Data submitted to ENA undergo automated quality checking and, where possible, manual curation. Data is made available via FTP and Aspera (a commmercial file transfer tool based on UDP).

In the MAGE-TAB submission for an experiment, raw files (fastq) are pointed to as URIs in the SRA or ENA, and derived files (wig, bed) are included as Derived Array Data Files. Example MAGE-TAB from ArrayExpress for E-GEOD-19786:
"Scan Name"	"Comment [ENA_RUN]"	"Comment [FASTQ_URI]"	"Derived Array Data File"
SRR037089	SRR037089	ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR037/SRR037089/SRR037089.fastq	GSM494291_concat_2608_1.wig

3. SRA (NCBI's Sequence Read Archive):

The SRA accepts data in different formats (fastq, sff, srf etc.) and convert them into an SRA format. They are currently working on a BAM loader as well.
They assign accessions for metadata objects, not for the data files.
Data download is via Aspera (a commercial file transfer tool based on UDP).

